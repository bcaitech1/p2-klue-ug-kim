{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ff0b9c-a9bf-4145-a02e-c5b0709fa508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n",
    "from load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f58ef4e-c41d-4b4e-a7ba-a69a212f6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d60f5552-a4cb-4cb8-b4eb-40f6a341f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f01d560f-090a-4b24-84a2-75484a4011d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entity_01</th>\n",
       "      <th>entity_02</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n",
       "      <td>랜드로버</td>\n",
       "      <td>자동차</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n",
       "      <td>민주당</td>\n",
       "      <td>27석</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n",
       "      <td>유럽 축구 연맹</td>\n",
       "      <td>UEFA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...</td>\n",
       "      <td>강수일</td>\n",
       "      <td>공격수</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...</td>\n",
       "      <td>람캄행</td>\n",
       "      <td>퍼쿤 씨 인트라팃</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...</td>\n",
       "      <td>사우디아라비아</td>\n",
       "      <td>2002년</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...</td>\n",
       "      <td>토요타</td>\n",
       "      <td>일본</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...</td>\n",
       "      <td>방덕룡</td>\n",
       "      <td>선무원종공신(宣武原從功臣)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...</td>\n",
       "      <td>LG전자</td>\n",
       "      <td>북미</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...</td>\n",
       "      <td>차영수</td>\n",
       "      <td>의원</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence entity_01  \\\n",
       "0     영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...      랜드로버   \n",
       "1     선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...       민주당   \n",
       "2     유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  유럽 축구 연맹   \n",
       "3     용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...       강수일   \n",
       "4     람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...       람캄행   \n",
       "...                                                 ...       ...   \n",
       "8995  2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...   사우디아라비아   \n",
       "8996  일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...       토요타   \n",
       "8997  방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...       방덕룡   \n",
       "8998  LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...      LG전자   \n",
       "8999  전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...       차영수   \n",
       "\n",
       "           entity_02  label  \n",
       "0                자동차     17  \n",
       "1                27석      0  \n",
       "2               UEFA      6  \n",
       "3                공격수      2  \n",
       "4          퍼쿤 씨 인트라팃      8  \n",
       "...              ...    ...  \n",
       "8995           2002년      0  \n",
       "8996              일본      9  \n",
       "8997  선무원종공신(宣武原從功臣)      2  \n",
       "8998              북미      0  \n",
       "8999              의원      2  \n",
       "\n",
       "[9000 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "# load dataset\n",
    "dataset = load_data(\"/opt/ml/input/data/train/train.tsv\")\n",
    "label = dataset['label'].values\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f573188-e7d8-44cf-9261-4dc46edf7c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([     0,      6,  76253, 244160,  11199,   1065,    294,  21290,    268,\n",
       "          35725, 244942, 105051,   8740, 195810,      3,      2,      2,      6,\n",
       "          76253, 244160,  11199,  76826,    697,    427,  10945,   2680,   1180,\n",
       "            427,  13330,   2680,  62657,   1020,  20047,  12412,    469,    367,\n",
       "          76826,  91368, 244942, 105051,   8740, 195810,      3,   1291,  61286,\n",
       "         136949,   7342,   1571,  62657,    367,      6, 135608,  79796, 129172,\n",
       "           1083,  48495,   2211, 135647,      5,      2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor(8)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dataset.iloc[4:6]\n",
    "temp\n",
    "temp2 = TokenDataset(temp, tokenizer)\n",
    "temp2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7604301d-b52b-4128-a598-f8f86884bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_datasets = self.tokenized_dataset(self.dataset, self.tokenizer)\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in tokenized_datasets.items()}\n",
    "        item['labels'] = torch.tensor(list(self.dataset['label'])[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def tokenized_dataset(self, dataset, tokenizer):\n",
    "        concat_entity = []\n",
    "        for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "            temp = ''\n",
    "            temp = e01 + '[SEP]' + e02\n",
    "            # temp = e01 + '</s></s>' + e02  # roberta\n",
    "            concat_entity.append(temp)\n",
    "        \n",
    "        tokenized_sentences = tokenizer(\n",
    "            concat_entity,\n",
    "            list(dataset['sentence']),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=100,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "507c83d5-e524-46d9-88d7-62902f4bec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=128\n",
    "batch_size=32\n",
    "warmup_ratio=0.01\n",
    "num_epochs=10\n",
    "max_grad_norm=1\n",
    "log_interval=50\n",
    "learning_rate=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "608bb280-4e2f-4c22-b600-5eb91938873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6999016-7c42-4555-be68-3a474c93f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_config = AutoConfig.from_pretrained('xlm-roberta-large')\n",
    "model_config.num_labels = 42\n",
    "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-large', config=model_config)\n",
    "\n",
    "model.parameters\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(temp2, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0559027-a0c8-40f0-81fb-c15c798d0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = LabelSmoothingLoss()\n",
    "\n",
    "t_total = len(train_loader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5245e99-1b47-4ac9-905c-9ceef8f00922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     0,    180,  20441,   1065,    294,  21290,    268, 107659,    713,\n",
      "              2,      2, 170744,  20448,   1963,  28211,  16069,    769,  59066,\n",
      "         153653, 109433,   2020,  21037,   4156,   8267, 162342, 160628,    132,\n",
      "            670,  20441,     16,    713,  32617,   1291, 136892,  62657,   1180,\n",
      "          23358,   1083,   3497,   2947,    713,   3626,  12057,  24788, 177441,\n",
      "          55388,  46431,  44928,   1077, 174878,      5,      2,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([0])}\n",
      "epoch!\n",
      "==========\n",
      "SequenceClassifierOutput(loss=tensor(3.4021, device='cuda:0', grad_fn=<NllLossBackward>), logits=tensor([[ 0.5150,  0.7255,  0.6829,  0.3777, -0.4498, -0.1628,  0.5972,  0.4204,\n",
      "          0.3260, -0.5926,  0.1211,  0.0347,  0.5355, -0.3474,  0.2313,  0.2954,\n",
      "          0.3751, -0.1068,  0.0941,  0.0309, -0.3492, -0.5730,  0.1716,  0.3061,\n",
      "          0.0727, -0.0841, -0.0490,  0.1789,  0.1605,  0.5009, -0.0613,  0.6079,\n",
      "          0.1575,  0.2141, -0.2209,  0.5078,  0.2601, -0.2823, -0.4474,  0.3932,\n",
      "         -0.3374,  0.2840]], device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "==========\n",
      "tensor(3.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "==========\n",
      "tensor([[ 0.5150,  0.7255,  0.6829,  0.3777, -0.4498, -0.1628,  0.5972,  0.4204,\n",
      "          0.3260, -0.5926,  0.1211,  0.0347,  0.5355, -0.3474,  0.2313,  0.2954,\n",
      "          0.3751, -0.1068,  0.0941,  0.0309, -0.3492, -0.5730,  0.1716,  0.3061,\n",
      "          0.0727, -0.0841, -0.0490,  0.1789,  0.1605,  0.5009, -0.0613,  0.6079,\n",
      "          0.1575,  0.2141, -0.2209,  0.5078,  0.2601, -0.2823, -0.4474,  0.3932,\n",
      "         -0.3374,  0.2840]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "==========\n",
      "tensor([0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'log_softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-779fb2760d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-0e8dc9bd93b5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtrue_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'log_softmax'"
     ]
    }
   ],
   "source": [
    "test = 0\n",
    "test1 = 0\n",
    "\n",
    "for batch in train_loader:\n",
    "        print(batch)\n",
    "        print(\"epoch!\")\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        print(\"=\" * 10)\n",
    "        print(outputs)\n",
    "        print(\"=\" * 10)\n",
    "        print(outputs[0])\n",
    "        print(\"=\" * 10)\n",
    "        print(outputs[1])\n",
    "        test = outputs[1]\n",
    "        print(\"=\" * 10)\n",
    "        print(labels)\n",
    "        test1 = labels\n",
    "        loss = loss_fn(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b5898aa-e501-449b-bfa1-2affdda3e975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4021, -3.1916, -3.2342, -3.5394, -4.3669, -4.0800, -3.3200, -3.4967,\n",
       "         -3.5912, -4.5097, -3.7961, -3.8824, -3.3816, -4.2645, -3.6858, -3.6217,\n",
       "         -3.5421, -4.0240, -3.8230, -3.8862, -4.2663, -4.4901, -3.7455, -3.6110,\n",
       "         -3.8444, -4.0012, -3.9661, -3.7382, -3.7566, -3.4162, -3.9785, -3.3092,\n",
       "         -3.7596, -3.7030, -4.1380, -3.4093, -3.6570, -4.1994, -4.3645, -3.5240,\n",
       "         -4.2545, -3.6332]], device='cuda:0', grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.log_softmax(test, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "250fc096-5182-40b6-938c-776dd36d0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "\tdef __init__(self, classes=42, smoothing=0.0, dim=-1):\n",
    "\t\tsuper(LabelSmoothingLoss, self).__init__()\n",
    "\t\tself.confidence = 1.0 - smoothing\n",
    "\t\tself.smoothing = smoothing\n",
    "\t\tself.cls = classes\n",
    "\t\tself.dim = dim\n",
    "\t\t\n",
    "\tdef forward(self, pred, target):\n",
    "\t\tpred = F.log_softmax(pred, dim=self.dim)\n",
    "\t\t# pred = pred.log_softmax(dim=self.dim)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ttrue_dist = torch.zeros_like(pred)\n",
    "\t\t\ttrue_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "\t\t\ttrue_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "\t\treturn torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f964c0a0-3a87-4879-adc4-a12b1b1073e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4021, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = LabelSmoothingLoss()\n",
    "print(loss_fn(test, test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13769c12-3f76-4a2d-bd8e-292d343f90fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5150,  0.7255,  0.6829,  0.3777, -0.4498, -0.1628,  0.5972,  0.4204,\n",
       "          0.3260, -0.5926,  0.1211,  0.0347,  0.5355, -0.3474,  0.2313,  0.2954,\n",
       "          0.3751, -0.1068,  0.0941,  0.0309, -0.3492, -0.5730,  0.1716,  0.3061,\n",
       "          0.0727, -0.0841, -0.0490,  0.1789,  0.1605,  0.5009, -0.0613,  0.6079,\n",
       "          0.1575,  0.2141, -0.2209,  0.5078,  0.2601, -0.2823, -0.4474,  0.3932,\n",
       "         -0.3374,  0.2840]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dad5868e-861b-4009-9aa6-322752c2a534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(3.4021, device='cuda:0', grad_fn=<NllLossBackward>), logits=tensor([[ 0.5150,  0.7255,  0.6829,  0.3777, -0.4498, -0.1628,  0.5972,  0.4204,\n",
       "          0.3260, -0.5926,  0.1211,  0.0347,  0.5355, -0.3474,  0.2313,  0.2954,\n",
       "          0.3751, -0.1068,  0.0941,  0.0309, -0.3492, -0.5730,  0.1716,  0.3061,\n",
       "          0.0727, -0.0841, -0.0490,  0.1789,  0.1605,  0.5009, -0.0613,  0.6079,\n",
       "          0.1575,  0.2141, -0.2209,  0.5078,  0.2601, -0.2823, -0.4474,  0.3932,\n",
       "         -0.3374,  0.2840]], device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e8db266-70f9-41dd-aefb-bad6a2c494a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a4404aa-fcbb-4f75-ad7c-af3c3e400167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e20cfd0-b3a9-42b5-8368-9ce1437344a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5651466d-3f2b-42cc-be83-0a417476471f",
   "metadata": {},
   "source": [
    "### Load model weights\n",
    "- nn.Parallel 사용할 때는 유의해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476ccdd-e73b-4615-bff2-ad2829108c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64feed02-2ba9-4ecd-a42a-6f09a080f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xlm-roberta-large\"\n",
    "model_config = XLMRobertaConfig.from_pretrained(model_name)\n",
    "model_config.num_labels = 42\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(model_name, config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf741f7-7b3a-4b52-bb5d-d8d70e09cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xlm-roberta-large\"\n",
    "model_config = XLMRobertaConfig.from_pretrained(model_name)\n",
    "model_config.num_labels = 42\n",
    "model = XLMRobertaForSequenceClassification(model_config)\n",
    "# model = XLMRobertaForSequenceClassification.from_pretrained(model_name, config=model_config)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(\"./checkpoints/expr/best_2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb74f0-79ac-4366-b20e-e828a9abf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "\n",
    "# load dataset\n",
    "train_dataset = load_data(\"/opt/ml/input/data/train/train2.tsv\")\n",
    "train_label = train_dataset['label'].values\n",
    "\n",
    "# tokenizing dataset\n",
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0971efc-42dd-42ff-860c-c2093885ba00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
