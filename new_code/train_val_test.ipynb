{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c9e1ea3-55b7-410b-b3bb-459a5db31053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n",
    "from load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30385989-04d7-43dd-94cb-8b890b2ffbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f470cd92-dae6-4f13-af3c-9c5634fd1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9875814-c377-4fca-9b0d-5f905def6fcb",
   "metadata": {},
   "source": [
    "### Train, Validation 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4d1646-fc59-4830-acef-6cc0236daf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entity_01</th>\n",
       "      <th>entity_02</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n",
       "      <td>랜드로버</td>\n",
       "      <td>자동차</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n",
       "      <td>민주당</td>\n",
       "      <td>27석</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n",
       "      <td>유럽 축구 연맹</td>\n",
       "      <td>UEFA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...</td>\n",
       "      <td>강수일</td>\n",
       "      <td>공격수</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...</td>\n",
       "      <td>람캄행</td>\n",
       "      <td>퍼쿤 씨 인트라팃</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...</td>\n",
       "      <td>사우디아라비아</td>\n",
       "      <td>2002년</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...</td>\n",
       "      <td>토요타</td>\n",
       "      <td>일본</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...</td>\n",
       "      <td>방덕룡</td>\n",
       "      <td>선무원종공신(宣武原從功臣)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...</td>\n",
       "      <td>LG전자</td>\n",
       "      <td>북미</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...</td>\n",
       "      <td>차영수</td>\n",
       "      <td>의원</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence entity_01  \\\n",
       "0     영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...      랜드로버   \n",
       "1     선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...       민주당   \n",
       "2     유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  유럽 축구 연맹   \n",
       "3     용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...       강수일   \n",
       "4     람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...       람캄행   \n",
       "...                                                 ...       ...   \n",
       "8995  2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...   사우디아라비아   \n",
       "8996  일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...       토요타   \n",
       "8997  방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...       방덕룡   \n",
       "8998  LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...      LG전자   \n",
       "8999  전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...       차영수   \n",
       "\n",
       "           entity_02  label  \n",
       "0                자동차     17  \n",
       "1                27석      0  \n",
       "2               UEFA      6  \n",
       "3                공격수      2  \n",
       "4          퍼쿤 씨 인트라팃      8  \n",
       "...              ...    ...  \n",
       "8995           2002년      0  \n",
       "8996              일본      9  \n",
       "8997  선무원종공신(宣武原從功臣)      2  \n",
       "8998              북미      0  \n",
       "8999              의원      2  \n",
       "\n",
       "[9000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "\n",
    "# load dataset\n",
    "dataset = load_data(\"/opt/ml/input/data/train/train.tsv\")\n",
    "label = dataset['label'].values\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bdeda0-fcd7-4ed8-880a-87b908377ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "        temp = ''\n",
    "        temp = e01 + '[SEP]' + e02\n",
    "        # temp = e01 + '</s></s>' + e02  # roberta\n",
    "        concat_entity.append(temp)\n",
    "    tokenized_sentences = tokenizer(\n",
    "        concat_entity,\n",
    "        list(dataset['sentence']),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a004b0f-da7a-4eca-a523-12ca301e1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de5ab429-708f-401c-ad35-8d71a3604a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "tokeinzed_val = tokenized_dataset(val_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d447639-1039-440a-b87d-c6e980a0ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_label = train_dataset['label'].values\n",
    "tokenized_val_label = val_dataset['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e483930-765f-4780-bade-9c874dd558d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      6,   7194,  ...,      1,      1,      1],\n",
       "        [     0,      6,  94320,  ...,      1,      1,      1],\n",
       "        [     0,  93278, 102384,  ...,      1,      1,      1],\n",
       "        ...,\n",
       "        [     0,   5920,  46157,  ...,      1,      1,      1],\n",
       "        [     0,      6, 128682,  ...,   7063,  56730,      2],\n",
       "        [     0,  68745,  11758,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "588d4313-188c-4071-b282-c1d3b80f3797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entity_01</th>\n",
       "      <th>entity_02</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>강용석 씨는 기자 출신 김세의(43) 씨, 김용호(44) 씨와 유튜브 채널 '가로세...</td>\n",
       "      <td>가세연</td>\n",
       "      <td>기자</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>무엇보다 벤타스 코어는 현대인들이 일상생활 중 가장 많은 피로를 호소하는 허리 부위...</td>\n",
       "      <td>벤타스 코어</td>\n",
       "      <td>허리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>하지만 1714년 왕위를 계승한 하노버 왕가의 조지 1세가 영어를 잘 구사하지 못했...</td>\n",
       "      <td>조지 1세</td>\n",
       "      <td>하노버 왕가</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4690</th>\n",
       "      <td>4월 5일 밤, 김규식, 홍명희, 김붕준 등과 경교장을 찾아 김구 숙소에서 협상 내...</td>\n",
       "      <td>홍명희</td>\n",
       "      <td>경교장</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8133</th>\n",
       "      <td>국민의당은 완전 국민 경선 80%, 여론조사 20%가 반영되는 경선을 통해 대한민국...</td>\n",
       "      <td>안철수</td>\n",
       "      <td>국민의당</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>성주군의 롯데 골프장을 주한미군 기지로 용도변경하여, 사드 1개 포대가 배치되었다.</td>\n",
       "      <td>주한미군 기지</td>\n",
       "      <td>사드</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8204</th>\n",
       "      <td>연합뉴스는 \"유서 조작 의혹은 노 전 대통령의 측근이 유서에 담긴 내용이라며 전한 ...</td>\n",
       "      <td>노</td>\n",
       "      <td>대통령</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>의학원은 전문의 및 간호사로 구성된 의료 지원단과 앰뷸런스를 대회 기간 전일 배치해...</td>\n",
       "      <td>LPGA</td>\n",
       "      <td>선수</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8675</th>\n",
       "      <td>윌리엄 1세의 지휘하에서, 에옹의 아들 중 3명 (알랑, 에티엔, 브리앙)이 리치먼...</td>\n",
       "      <td>알랑</td>\n",
       "      <td>리치먼드</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>이때 자유당의 실력자 1순위에 올랐던 것은 박마리아의 남편 이기붕과 족청(조선민족청...</td>\n",
       "      <td>조선민족청년단</td>\n",
       "      <td>이범석</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence entity_01 entity_02  \\\n",
       "5766  강용석 씨는 기자 출신 김세의(43) 씨, 김용호(44) 씨와 유튜브 채널 '가로세...       가세연        기자   \n",
       "4245  무엇보다 벤타스 코어는 현대인들이 일상생활 중 가장 많은 피로를 호소하는 허리 부위...    벤타스 코어        허리   \n",
       "6092  하지만 1714년 왕위를 계승한 하노버 왕가의 조지 1세가 영어를 잘 구사하지 못했...     조지 1세    하노버 왕가   \n",
       "4690  4월 5일 밤, 김규식, 홍명희, 김붕준 등과 경교장을 찾아 김구 숙소에서 협상 내...       홍명희       경교장   \n",
       "8133  국민의당은 완전 국민 경선 80%, 여론조사 20%가 반영되는 경선을 통해 대한민국...       안철수      국민의당   \n",
       "1329     성주군의 롯데 골프장을 주한미군 기지로 용도변경하여, 사드 1개 포대가 배치되었다.   주한미군 기지        사드   \n",
       "8204  연합뉴스는 \"유서 조작 의혹은 노 전 대통령의 측근이 유서에 담긴 내용이라며 전한 ...         노       대통령   \n",
       "1684  의학원은 전문의 및 간호사로 구성된 의료 지원단과 앰뷸런스를 대회 기간 전일 배치해...      LPGA        선수   \n",
       "8675  윌리엄 1세의 지휘하에서, 에옹의 아들 중 3명 (알랑, 에티엔, 브리앙)이 리치먼...        알랑      리치먼드   \n",
       "5795  이때 자유당의 실력자 1순위에 올랐던 것은 박마리아의 남편 이기붕과 족청(조선민족청...   조선민족청년단       이범석   \n",
       "\n",
       "      label  \n",
       "5766      0  \n",
       "4245      0  \n",
       "6092      4  \n",
       "4690      0  \n",
       "8133      4  \n",
       "1329      0  \n",
       "8204      2  \n",
       "1684      0  \n",
       "8675      0  \n",
       "5795     10  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.iloc[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "143e3e42-1e5a-4cb5-844e-c2737c04cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 주한미군 기지[SEP]사드</s></s> 성주군의 롯데 골프장을 주한미군 기지로 용도변경하여, 사드 1개 포대가 배치되었다.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "label:  0\n"
     ]
    }
   ],
   "source": [
    "idx = 105\n",
    "\n",
    "temp = tokenized_train.input_ids[idx]\n",
    "print(tokenizer.decode(temp))\n",
    "print(\"label: \", tokenized_train_label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f8d2e-7638-4d56-994f-551c3454d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_datasets = self.tokenized_dataset(self.dataset, self.tokenizer)\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in tokenized_datasets.items()}\n",
    "        item['labels'] = torch.tensor(list(self.dataset['label'])[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def tokenized_dataset(self, dataset, tokenizer):\n",
    "        concat_entity = []\n",
    "        for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "            temp = ''\n",
    "            temp = e01 + '[SEP]' + e02\n",
    "            # temp = e01 + '</s></s>' + e02  # roberta\n",
    "            concat_entity.append(taemp)\n",
    "        \n",
    "        tokenized_sentences = tokenizer(\n",
    "            concat_entity,\n",
    "            list(dataset['sentence']),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=100,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "14ad216c-1ccc-4278-843e-43d050d6be99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entity_01</th>\n",
       "      <th>entity_02</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>무하의 애국심은 히틀러의 독일 정부가 그를의 적으로 간주한 명부에 포함할 만큼 유명했다.</td>\n",
       "      <td>히틀러</td>\n",
       "      <td>독일</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>위나라가 퇴각하면서 일대의 주민과 가축을 몽땅 데려가 버렸기 때문에 무도 음평에 주...</td>\n",
       "      <td>촉군</td>\n",
       "      <td>위나라</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>이는 삼성그룹 회장인 이건희가 아들인 이재용에게 경영권을 인계하여 에버랜드-삼성생명...</td>\n",
       "      <td>삼성카드</td>\n",
       "      <td>삼성그룹</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7850</th>\n",
       "      <td>청원인은 'TJB 대전방송'에서 6년을 근무하며 그중 3년 6개월을 '8뉴스' 앵커...</td>\n",
       "      <td>대전방송</td>\n",
       "      <td>TJB</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>저명한 교수로는 물리학자 에드워드 빅터 애플턴, 오언 윌런스 리처드슨, 물리학자/수...</td>\n",
       "      <td>찰스 라이엘</td>\n",
       "      <td>지질학</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734</th>\n",
       "      <td>그러나 조지 6세 전 영국 국왕이 초대 수장을 역임한 이래 엘리자베스 2세 현 여왕...</td>\n",
       "      <td>엘리자베스 2세</td>\n",
       "      <td>조지 6세</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>소설은 제2차 세계 대전을 배경으로 가상의 미국 육군 항공대 B-25 폭격기 편대의...</td>\n",
       "      <td>미국 육군 항공대</td>\n",
       "      <td>2차 세계 대전</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>그는 1975년과 1976년 레즈와 함께 2회의 월드 시리즈를 우승하였고, 각각의 ...</td>\n",
       "      <td>내셔널 리그</td>\n",
       "      <td>1975년</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>법무부 장관으로 2014년 신고한 재산내역은 모두 25억8925만원이었으며 법무부 ...</td>\n",
       "      <td>민주노동당</td>\n",
       "      <td>통합진보당</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>2004년 4월 창단한 홍천 이두 FC는 2006년 전라남도 여수시를 연고지로 내셔...</td>\n",
       "      <td>홍천 이두 FC</td>\n",
       "      <td>내셔널리그</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  entity_01 entity_02  \\\n",
       "6317  무하의 애국심은 히틀러의 독일 정부가 그를의 적으로 간주한 명부에 포함할 만큼 유명했다.        히틀러        독일   \n",
       "740   위나라가 퇴각하면서 일대의 주민과 가축을 몽땅 데려가 버렸기 때문에 무도 음평에 주...         촉군       위나라   \n",
       "3781  이는 삼성그룹 회장인 이건희가 아들인 이재용에게 경영권을 인계하여 에버랜드-삼성생명...       삼성카드      삼성그룹   \n",
       "7850  청원인은 'TJB 대전방송'에서 6년을 근무하며 그중 3년 6개월을 '8뉴스' 앵커...       대전방송       TJB   \n",
       "2963  저명한 교수로는 물리학자 에드워드 빅터 애플턴, 오언 윌런스 리처드슨, 물리학자/수...     찰스 라이엘       지질학   \n",
       "...                                                 ...        ...       ...   \n",
       "5734  그러나 조지 6세 전 영국 국왕이 초대 수장을 역임한 이래 엘리자베스 2세 현 여왕...   엘리자베스 2세     조지 6세   \n",
       "5191  소설은 제2차 세계 대전을 배경으로 가상의 미국 육군 항공대 B-25 폭격기 편대의...  미국 육군 항공대  2차 세계 대전   \n",
       "5390  그는 1975년과 1976년 레즈와 함께 2회의 월드 시리즈를 우승하였고, 각각의 ...     내셔널 리그     1975년   \n",
       "860   법무부 장관으로 2014년 신고한 재산내역은 모두 25억8925만원이었으며 법무부 ...      민주노동당     통합진보당   \n",
       "7270  2004년 4월 창단한 홍천 이두 FC는 2006년 전라남도 여수시를 연고지로 내셔...   홍천 이두 FC     내셔널리그   \n",
       "\n",
       "      label  \n",
       "6317      7  \n",
       "740       0  \n",
       "3781      3  \n",
       "7850      6  \n",
       "2963      2  \n",
       "...     ...  \n",
       "5734      0  \n",
       "5191      0  \n",
       "5390      0  \n",
       "860       0  \n",
       "7270     15  \n",
       "\n",
       "[7200 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4996cf19-dd67-4e39-8691-f6f6cff49aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([     0,  93278, 102384,   1065,    294,  21290,    268,  53885,   3276,\n",
      "        121980,      2,      2,  93172,  93278, 121980,  99862,   1571, 199323,\n",
      "         23423,    713, 129172,   1571,   1504,   7046,   5358,   9913,  92189,\n",
      "        119974,   8740,  11931,   8016,  29064,  15164,  65575,      9,  53885,\n",
      "          3276, 199497,      9,  53885,   3276, 102384,      9,  53885,   3276,\n",
      "        112242,      9,  29064,  15164,  65575,   1083,  30297,  21252,      6,\n",
      "        229744,  14842,   2268,    480, 151908,  95026, 216503,    688, 133741,\n",
      "         92052,  39649,  68682, 126296,      6,  50751,      5,      2,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]), 'labels': tensor(3)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> 삼성카드[SEP]삼성그룹</s></s> 이는 삼성그룹 회장인 이건희가 아들인 이재용에게 경영권을 인계하여 에버랜드-삼성생명-삼성카드-삼성전자- 에버랜드로 이어지는 순환출자에 의한 그룹 지배를 확보하는데 있어 중요한 역할을 하였다.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets = TokenDataset(train_dataset, tokenizer)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "ids = train_datasets[2]['input_ids']\n",
    "print(train_datasets[2])\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c10b0d-a5a1-4eb1-ba8d-0907059e24d4",
   "metadata": {},
   "source": [
    "### K-fold 구현하기\n",
    "- 가능하면 Stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "02f2de35-bd2f-446b-811f-51b8a46236ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds=5\n",
    "kfold = KFold(n_splits=k_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4e519159-34db-44a4-aa44-000ec513667a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "10ad0c75-28e9-4ca0-9a1b-5f1dadb2bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  [1800 1801 1802 ... 8997 8998 8999]\n",
      "val:  [   0    1    2 ... 1797 1798 1799]\n",
      "train:  [   0    1    2 ... 8997 8998 8999]\n",
      "val:  [1800 1801 1802 ... 3597 3598 3599]\n",
      "train:  [   0    1    2 ... 8997 8998 8999]\n",
      "val:  [3600 3601 3602 ... 5397 5398 5399]\n",
      "train:  [   0    1    2 ... 8997 8998 8999]\n",
      "val:  [5400 5401 5402 ... 7197 7198 7199]\n",
      "train:  [   0    1    2 ... 7197 7198 7199]\n",
      "val:  [7200 7201 7202 ... 8997 8998 8999]\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "        print(\"train: \", train_ids)\n",
    "        print(\"val: \", val_ids)\n",
    "        \n",
    "        temp = dataset.iloc[train_ids]\n",
    "#         print(\"temp: \", temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd437b-49c0-4d04-a121-626a7abd66fc",
   "metadata": {},
   "source": [
    "### Trainer 안쓰고 trainingmax_len=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c0dacb0b-324b-4594-82e9-f3925fec9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=128\n",
    "batch_size=32\n",
    "warmup_ratio=0.01\n",
    "num_epochs=10\n",
    "max_grad_norm=1\n",
    "log_interval=50\n",
    "learning_rate=5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e34b313b-206b-4d0d-bc5a-46db5a35b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "610dcede-02c4-454e-a901-876e15ef443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_config = AutoConfig.from_pretrained('xlm-roberta-large')\n",
    "model_config.num_labels = 42\n",
    "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-large', config=model_config)\n",
    "\n",
    "model.parameters\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_datasets, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ef4fb7e1-6f2c-497a-a638-c625b49d29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = LabelSmoothingLoss()\n",
    "\n",
    "t_total = len(train_loader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "34d48158-806f-4b91-bb38-727310155c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0, 203559,   7849,  ...,      1,      1,      1],\n",
       "         [     0,  21486,    480,  ...,      1,      1,      1],\n",
       "         [     0,   8237,  23224,  ...,      1,      1,      1],\n",
       "         ...,\n",
       "         [     0,   7463,   3013,  ...,      1,      1,      1],\n",
       "         [     0,     15,   2688,  ...,      1,      1,      1],\n",
       "         [     0,   1504,  20126,  ...,      1,      1,      1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([ 0,  0,  1,  2,  0, 20,  0, 15,  0,  2,  0,  4,  0, 10,  0, 15, 15, 15,\n",
       "          0,  0,  0,  0,  0,  4,  0,  9,  0,  0,  2,  0, 10,  0])}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "19a8b569-65af-4171-bbe6-cb80363e8006",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'bos_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-4415d0c7ff0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBERTSentenceTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, max_seq_length, vocab, pad, pair)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cls_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cls_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sep_token'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sep_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'bos_token'"
     ]
    }
   ],
   "source": [
    "nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=128, pad=True, pair=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ae2d06e0-a020-46ff-962a-aca19c2a119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     0,   8511,  12431,  ...,      1,      1,      1],\n",
      "        [     0,      6, 231016,  ...,      1,      1,      1],\n",
      "        [     0,      6, 132467,  ...,      1,      1,      1],\n",
      "        ...,\n",
      "        [     0,   3626,  13902,  ...,      1,      1,      1],\n",
      "        [     0,   8237,  26870,  ...,      1,      1,      1],\n",
      "        [     0,  14592,  27082,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([ 4, 10, 15, 10,  0,  6,  8,  0,  0,  0, 25,  0,  0,  0,  2, 10, 17, 10,\n",
      "         0,  0,  0,  0,  2, 10,  0,  0,  0,  0, 35,  0, 27,  4])}\n",
      "epoch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/225 [00:19<1:11:30, 19.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     0,   7592,  12412,  ...,      1,      1,      1],\n",
      "        [     0,      6, 217783,  ...,      1,      1,      1],\n",
      "        [     0,  20294,   5821,  ...,   5476,      5,      2],\n",
      "        ...,\n",
      "        [     0, 128300,   1065,  ...,      1,      1,      1],\n",
      "        [     0,   5241,   8443,  ...,      1,      1,      1],\n",
      "        [     0,  41716,   1083,  ...,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([ 0, 10,  0, 10, 13,  0,  2,  0,  4,  0,  0,  0,  4,  2,  0,  4,  0,  0,\n",
      "         0,  0,  0, 24, 29,  0,  2,  0,  0,  0, 22,  0, 36, 26])}\n",
      "epoch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/225 [00:47<1:28:51, 23.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-cabe9daa4f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-329c23d34663>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtokenized_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-329c23d34663>\u001b[0m in \u001b[0;36mtokenized_dataset\u001b[0;34m(self, dataset, tokenizer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m             )\n\u001b[1;32m   2331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2512\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2514\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2515\u001b[0m         )\n\u001b[1;32m   2516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             )\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         ]\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             )\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         ]\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_convert_encoding\u001b[0;34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mencoding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"special_tokens_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_tokens_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in tqdm(train_loader):\n",
    "        print(batch)\n",
    "        print(\"epoch!\")\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a722bdf-2f6b-4e2c-89d2-7dfdc372cb56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
